{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape all initial websites to train the module\n",
    "with open(\"../dbHandler/iab_categories_initial_data.json\", \"r\") as jsonobj:\n",
    "    initial_data_json = json.load(jsonobj)\n",
    "\n",
    "website_array = []\n",
    "websites_to_train_content = {}\n",
    "\n",
    "for item in initial_data_json[\"iabCategories\"].values():\n",
    "    website_array.extend(item[\"websites\"])\n",
    "\n",
    "total_websites = len(website_array)\n",
    "website_loading_index = 0\n",
    "\n",
    "for website in website_array:\n",
    "    print(f\"loading {website_loading_index}/{total_websites} ======> {website}\")\n",
    "    website_loading_index = website_loading_index + 1\n",
    "\n",
    "    with open(\"../scrapeContent/website_to_scrape.txt\", \"w\") as file:\n",
    "        file.write(website)\n",
    "\n",
    "    %run \"../scrapeContent/scrapeContent.ipynb\"\n",
    "    #%run \"../dbHandler/addNewLanguageKeywords.ipynb\" #the initial websites will be all in english, if not the keywords should all be grabbed from the database after running this line\n",
    "    \n",
    "    with open(\"../scrapeContent/scraped_content_output.json\", \"r\") as scrapedContentJsonObj:\n",
    "        scraped_content_output = json.load(scrapedContentJsonObj)\n",
    "\n",
    "    #create new json where all the websites are present\n",
    "    websites_to_train_content[website] = {\"description\":scraped_content_output[\"description\"], \"alt_images_texts\": scraped_content_output[\"alt_images_texts\"], \"content\": scraped_content_output[\"content\"],\"status_code\":scraped_content_output[\"status_code\"]}\n",
    "\n",
    "print(f\"loading {website_loading_index}/{total_websites}\")\n",
    "#this is only if we want to see the content of the json above\n",
    "#with open(\"../scrapeContent/websites_to_train_content.json\", \"w\") as outputFile:\n",
    "#    json.dump(websites_to_train_content, outputFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the json above, into trainig data, filtering it with all keywords \n",
    "processed_website_content = {}\n",
    "\n",
    "for website, contents in websites_to_train_content.items():\n",
    "    keywords_array = []\n",
    "\n",
    "    #grab keywords and the website category_name\n",
    "    for item in initial_data_json[\"iabCategories\"].values():\n",
    "        keywords_array.extend(item[\"keywords\"])\n",
    "\n",
    "        if website in item[\"websites\"]:\n",
    "            category_name = item[\"name\"]\n",
    "\n",
    "     #grab main_title assuming the first element is always the title in the head of the page\n",
    "    main_title = contents.get(\"content\", [])[0][\"text\"] if contents.get(\"content\") else \"\"\n",
    "    \n",
    "    #initiate processed_website_content json\n",
    "    processed_website_content[website] = {\n",
    "        \"status_code\": contents.get(\"status_code\", \"\"),\n",
    "        \"category_name\": category_name,\n",
    "        \"head_title\": main_title,\n",
    "        \"description\": contents.get(\"description\", \"\"),\n",
    "        \"alt_images_texts\": \"\"  \n",
    "    }\n",
    "\n",
    "    #store all alt_image_text s in an array\n",
    "    alt_images_texts_array = contents.get(\"alt_images_texts\", \"\")\n",
    "\n",
    "    #filter the text with the keywords and store all together in a string based on the elements there are in\n",
    "    for content in contents.get(\"content\", [])[1:]:\n",
    "        element = content[\"element\"]\n",
    "\n",
    "        \n",
    "        if element not in processed_website_content[website]:\n",
    "            processed_website_content[website][element] = \"\"\n",
    "        processed_website_content[website][element] += content[\"text\"] + \" \"\n",
    "\n",
    "    #filter alt_images_texts_array values with the keywords and store all in a string\n",
    "    for alt_text in alt_images_texts_array:\n",
    "        \n",
    "        processed_website_content[website][\"alt_images_texts\"] += alt_text + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the json in a file\n",
    "with open(\"../scrapeContent/processed_websites_content.json\", \"w\") as outputFile:\n",
    "    json.dump(processed_website_content, outputFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
